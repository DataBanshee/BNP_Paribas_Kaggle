{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of my Dataframe is 114321 x 133\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>...</th>\n",
       "      <th>v121</th>\n",
       "      <th>v122</th>\n",
       "      <th>v123</th>\n",
       "      <th>v124</th>\n",
       "      <th>v126</th>\n",
       "      <th>v127</th>\n",
       "      <th>v128</th>\n",
       "      <th>v129</th>\n",
       "      <th>v130</th>\n",
       "      <th>v131</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>114321.000000</td>\n",
       "      <td>114321.000000</td>\n",
       "      <td>6.448900e+04</td>\n",
       "      <td>6.452500e+04</td>\n",
       "      <td>6.452500e+04</td>\n",
       "      <td>6.569700e+04</td>\n",
       "      <td>6.448900e+04</td>\n",
       "      <td>6.448900e+04</td>\n",
       "      <td>6.570200e+04</td>\n",
       "      <td>6.447000e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>6.448100e+04</td>\n",
       "      <td>6.447000e+04</td>\n",
       "      <td>63643.000000</td>\n",
       "      <td>6.570200e+04</td>\n",
       "      <td>6.448900e+04</td>\n",
       "      <td>6.448900e+04</td>\n",
       "      <td>6.569700e+04</td>\n",
       "      <td>114321.000000</td>\n",
       "      <td>6.447800e+04</td>\n",
       "      <td>6.442600e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>114228.928228</td>\n",
       "      <td>0.761199</td>\n",
       "      <td>1.630686e+00</td>\n",
       "      <td>7.464411e+00</td>\n",
       "      <td>4.145098e+00</td>\n",
       "      <td>8.742359e+00</td>\n",
       "      <td>2.436402e+00</td>\n",
       "      <td>2.483921e+00</td>\n",
       "      <td>1.496569e+00</td>\n",
       "      <td>9.031859e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.737596e+00</td>\n",
       "      <td>6.822439e+00</td>\n",
       "      <td>3.549938</td>\n",
       "      <td>9.198120e-01</td>\n",
       "      <td>1.672658e+00</td>\n",
       "      <td>3.239542e+00</td>\n",
       "      <td>2.030373e+00</td>\n",
       "      <td>0.310144</td>\n",
       "      <td>1.925763e+00</td>\n",
       "      <td>1.739389e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>65934.487362</td>\n",
       "      <td>0.426353</td>\n",
       "      <td>1.082813e+00</td>\n",
       "      <td>2.961676e+00</td>\n",
       "      <td>1.148263e+00</td>\n",
       "      <td>2.036018e+00</td>\n",
       "      <td>5.999653e-01</td>\n",
       "      <td>5.894485e-01</td>\n",
       "      <td>2.783003e+00</td>\n",
       "      <td>1.930262e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.356294e+00</td>\n",
       "      <td>1.795978e+00</td>\n",
       "      <td>2.604704</td>\n",
       "      <td>2.099407e+00</td>\n",
       "      <td>5.031683e-01</td>\n",
       "      <td>1.625988e+00</td>\n",
       "      <td>1.074232e+00</td>\n",
       "      <td>0.693262</td>\n",
       "      <td>1.264497e+00</td>\n",
       "      <td>1.134702e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.996497e-07</td>\n",
       "      <td>-9.817614e-07</td>\n",
       "      <td>-6.475929e-07</td>\n",
       "      <td>-5.287068e-07</td>\n",
       "      <td>-9.055091e-07</td>\n",
       "      <td>-9.468765e-07</td>\n",
       "      <td>-7.783778e-07</td>\n",
       "      <td>-9.828757e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.820642e-07</td>\n",
       "      <td>-9.978497e-07</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>-9.994953e-07</td>\n",
       "      <td>-9.564174e-07</td>\n",
       "      <td>-9.223798e-07</td>\n",
       "      <td>8.197812e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.901257e-07</td>\n",
       "      <td>-9.999134e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57280.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.135798e-01</td>\n",
       "      <td>5.316428e+00</td>\n",
       "      <td>3.487398e+00</td>\n",
       "      <td>7.605918e+00</td>\n",
       "      <td>2.065064e+00</td>\n",
       "      <td>2.101477e+00</td>\n",
       "      <td>8.658986e-02</td>\n",
       "      <td>7.853659e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.786965e+00</td>\n",
       "      <td>5.647712e+00</td>\n",
       "      <td>1.963315</td>\n",
       "      <td>2.053777e-02</td>\n",
       "      <td>1.417600e+00</td>\n",
       "      <td>2.101900e+00</td>\n",
       "      <td>1.393830e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.106172e+00</td>\n",
       "      <td>1.012658e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>114189.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.469550e+00</td>\n",
       "      <td>7.023803e+00</td>\n",
       "      <td>4.205991e+00</td>\n",
       "      <td>8.670867e+00</td>\n",
       "      <td>2.412790e+00</td>\n",
       "      <td>2.452166e+00</td>\n",
       "      <td>3.860317e-01</td>\n",
       "      <td>9.059582e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.436195e+00</td>\n",
       "      <td>6.749117e+00</td>\n",
       "      <td>2.739239</td>\n",
       "      <td>1.398639e-01</td>\n",
       "      <td>1.614802e+00</td>\n",
       "      <td>2.963620e+00</td>\n",
       "      <td>1.798436e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.560138e+00</td>\n",
       "      <td>1.589403e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>171206.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.136128e+00</td>\n",
       "      <td>9.465497e+00</td>\n",
       "      <td>4.833250e+00</td>\n",
       "      <td>9.771353e+00</td>\n",
       "      <td>2.775285e+00</td>\n",
       "      <td>2.834285e+00</td>\n",
       "      <td>1.625246e+00</td>\n",
       "      <td>1.023256e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.379175e+00</td>\n",
       "      <td>7.911392e+00</td>\n",
       "      <td>4.075361</td>\n",
       "      <td>8.718333e-01</td>\n",
       "      <td>1.843886e+00</td>\n",
       "      <td>4.108146e+00</td>\n",
       "      <td>2.390158e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.332425e+00</td>\n",
       "      <td>2.261905e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>228713.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>19.686069</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>1.563161e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>2.000000e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID         target            v1            v2            v4  \\\n",
       "count  114321.000000  114321.000000  6.448900e+04  6.452500e+04  6.452500e+04   \n",
       "mean   114228.928228       0.761199  1.630686e+00  7.464411e+00  4.145098e+00   \n",
       "std     65934.487362       0.426353  1.082813e+00  2.961676e+00  1.148263e+00   \n",
       "min         3.000000       0.000000 -9.996497e-07 -9.817614e-07 -6.475929e-07   \n",
       "25%     57280.000000       1.000000  9.135798e-01  5.316428e+00  3.487398e+00   \n",
       "50%    114189.000000       1.000000  1.469550e+00  7.023803e+00  4.205991e+00   \n",
       "75%    171206.000000       1.000000  2.136128e+00  9.465497e+00  4.833250e+00   \n",
       "max    228713.000000       1.000000  2.000000e+01  2.000000e+01  2.000000e+01   \n",
       "\n",
       "                 v5            v6            v7            v8            v9  \\\n",
       "count  6.569700e+04  6.448900e+04  6.448900e+04  6.570200e+04  6.447000e+04   \n",
       "mean   8.742359e+00  2.436402e+00  2.483921e+00  1.496569e+00  9.031859e+00   \n",
       "std    2.036018e+00  5.999653e-01  5.894485e-01  2.783003e+00  1.930262e+00   \n",
       "min   -5.287068e-07 -9.055091e-07 -9.468765e-07 -7.783778e-07 -9.828757e-07   \n",
       "25%    7.605918e+00  2.065064e+00  2.101477e+00  8.658986e-02  7.853659e+00   \n",
       "50%    8.670867e+00  2.412790e+00  2.452166e+00  3.860317e-01  9.059582e+00   \n",
       "75%    9.771353e+00  2.775285e+00  2.834285e+00  1.625246e+00  1.023256e+01   \n",
       "max    2.000000e+01  2.000000e+01  2.000000e+01  2.000000e+01  2.000000e+01   \n",
       "\n",
       "           ...               v121          v122          v123          v124  \\\n",
       "count      ...       6.448100e+04  6.447000e+04  63643.000000  6.570200e+04   \n",
       "mean       ...       2.737596e+00  6.822439e+00      3.549938  9.198120e-01   \n",
       "std        ...       1.356294e+00  1.795978e+00      2.604704  2.099407e+00   \n",
       "min        ...      -9.820642e-07 -9.978497e-07      0.019139 -9.994953e-07   \n",
       "25%        ...       1.786965e+00  5.647712e+00      1.963315  2.053777e-02   \n",
       "50%        ...       2.436195e+00  6.749117e+00      2.739239  1.398639e-01   \n",
       "75%        ...       3.379175e+00  7.911392e+00      4.075361  8.718333e-01   \n",
       "max        ...       2.000000e+01  2.000000e+01     19.686069  2.000000e+01   \n",
       "\n",
       "               v126          v127          v128           v129          v130  \\\n",
       "count  6.448900e+04  6.448900e+04  6.569700e+04  114321.000000  6.447800e+04   \n",
       "mean   1.672658e+00  3.239542e+00  2.030373e+00       0.310144  1.925763e+00   \n",
       "std    5.031683e-01  1.625988e+00  1.074232e+00       0.693262  1.264497e+00   \n",
       "min   -9.564174e-07 -9.223798e-07  8.197812e-07       0.000000 -9.901257e-07   \n",
       "25%    1.417600e+00  2.101900e+00  1.393830e+00       0.000000  1.106172e+00   \n",
       "50%    1.614802e+00  2.963620e+00  1.798436e+00       0.000000  1.560138e+00   \n",
       "75%    1.843886e+00  4.108146e+00  2.390158e+00       0.000000  2.332425e+00   \n",
       "max    1.563161e+01  2.000000e+01  2.000000e+01      11.000000  2.000000e+01   \n",
       "\n",
       "               v131  \n",
       "count  6.442600e+04  \n",
       "mean   1.739389e+00  \n",
       "std    1.134702e+00  \n",
       "min   -9.999134e-07  \n",
       "25%    1.012658e+00  \n",
       "50%    1.589403e+00  \n",
       "75%    2.261905e+00  \n",
       "max    2.000000e+01  \n",
       "\n",
       "[8 rows x 114 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"ID\"].head(10) # The target column is to be predicted\n",
    "                 # Do I need to keep the ID? No real info brought?\n",
    "    \n",
    "print 'The shape of my Dataframe is {0} x {1}' .format(df.shape[0],df.shape[1])\n",
    "\n",
    "df_train.describe() # all the data counts for 114,321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of my Dataframe is 114393 x 132\n",
      " Size of the full data DataFrame 17651 x 132\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of null values per row</th>\n",
       "      <th>Count of Rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>17651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>28487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>14308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>1747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>25</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>65</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>66</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>81</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>82</td>\n",
       "      <td>1077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>83</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>84</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>85</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100</td>\n",
       "      <td>4338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>27038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102</td>\n",
       "      <td>15269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>103</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>104</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>105</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>106</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>107</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>108</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>109</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of null values per row  Count of Rows\n",
       "2                               0          17651\n",
       "0                               1          28487\n",
       "4                               2          14308\n",
       "6                               3           1747\n",
       "7                               4           1335\n",
       "20                              5             34\n",
       "17                              6             43\n",
       "25                              7             15\n",
       "26                              8              9\n",
       "33                              9              2\n",
       "36                             10              1\n",
       "37                             11              1\n",
       "30                             12              5\n",
       "29                             13              5\n",
       "45                             16              1\n",
       "38                             17              1\n",
       "39                             18              1\n",
       "10                             25            583\n",
       "13                             26            249\n",
       "21                             27             29\n",
       "32                             28              3\n",
       "43                             30              1\n",
       "35                             31              2\n",
       "44                             36              1\n",
       "28                             64              8\n",
       "22                             65             22\n",
       "27                             66              8\n",
       "40                             68              1\n",
       "12                             81            453\n",
       "8                              82           1077\n",
       "11                             83            458\n",
       "18                             84             41\n",
       "19                             85             40\n",
       "41                             88              1\n",
       "42                             89              1\n",
       "5                             100           4338\n",
       "1                             101          27038\n",
       "3                             102          15269\n",
       "9                             103            720\n",
       "14                            104            238\n",
       "15                            105             78\n",
       "24                            106             16\n",
       "16                            107             45\n",
       "23                            108             22\n",
       "34                            109              2\n",
       "31                            110              3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the data from the test set\n",
    "# We will extract the rows which have no NaN values\n",
    "\n",
    "df_test.describe() # all the data counts for 114,321\n",
    "\n",
    "print 'The shape of my Dataframe is {0} x {1}' .format(df_test.shape[0],df_test.shape[1])\n",
    "\n",
    "x_test = df_test.isnull().values\n",
    "\n",
    "# print(x[0:5])\n",
    "\n",
    "x_test = x_test.sum(axis=1)\n",
    "\n",
    "extract_full_rows = (x_test == 0)\n",
    "\n",
    "full_data = df_test.loc[extract_full_rows,:]\n",
    "print(\" Size of the full data DataFrame {0} x {1}\".format(full_data.shape[0], full_data.shape[1]))\n",
    "\n",
    "\n",
    "# Number of rows with n non null values\n",
    "# Could it be possible to use it as a feature \n",
    "# for Machine Learning?\n",
    "\n",
    "x_test = pd.Series(x_test)\n",
    "count_df_test = pd.DataFrame(x_test.value_counts())\n",
    "\n",
    "count_df_test = count_df_test.reset_index()\n",
    "count_df_test.columns = [\"Number of null values per row\",\"Count of Rows\"]\n",
    "count_df_test = count_df_test.sort_values([\"Number of null values per row\"], ascending=True)\n",
    "count_df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train[\"v110\"].head(10)\n",
    "\n",
    "df_types = df_train.dtypes\n",
    "\n",
    "# My non numerical values\n",
    "non_num_columns = df_types[df_types == \"object\"].index\n",
    "\n",
    "# My numerical values\n",
    "num_columns = df_types[df_types != \"object\"].index\n",
    "\n",
    "df_non_num = df_train.loc[:,non_num_columns]\n",
    "\n",
    "# pd.concat([df_non_num,df[\"target\"]],axis=1)\n",
    "# df[\"v22\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Size of the full data DataFrame 17756 x 133\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def turn_categorical(x):\n",
    "    \n",
    "    dict_v3 = {\"A\":1,\"B\":2,\"C\":3}\n",
    "    try:\n",
    "        return dict_v3[x]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Let's compute the number of intances which are missing per field\n",
    "\n",
    "# df[\"v2\"].apply(np.isnan).sum()\n",
    "\n",
    "df_train.loc[df[\"v1\"].apply(np.isnan)].head(10)\n",
    "\n",
    "# Let's see how v3 categorical value correlate with the target values\n",
    "# What kind of calues do I have in v3:\n",
    "\n",
    "df_train[\"v3\"].value_counts() # A : 227\n",
    "                        # B : 53\n",
    "                        # C : 110,584\n",
    "        \n",
    "\n",
    "\n",
    "serie_target = pd.Series(df_train[\"v3\"].map(lambda x: turn_categorical(x)))\n",
    "serie_target\n",
    "\n",
    "\n",
    "#plt.scatter(serie_target,df[\"\"])\n",
    "\n",
    "x = df_train.isnull().values\n",
    "\n",
    "# print(x[0:5])\n",
    "\n",
    "x = x.sum(axis=1)\n",
    "\n",
    "extract_full_rows = (x == 0)\n",
    "\n",
    "full_data = df_train.loc[extract_full_rows,:]\n",
    "print(\" Size of the full data DataFrame {0} x {1}\".format(full_data.shape[0], full_data.shape[1]))\n",
    "\n",
    "\n",
    "# Number of rows with n non null values\n",
    "# Could it be possible to use it as a feature \n",
    "# for Machine Learning?\n",
    "\n",
    "x = pd.Series(x)\n",
    "count_df = pd.DataFrame(x.value_counts())\n",
    "\n",
    "count_df = count_df.reset_index()\n",
    "count_df.columns = [\"Number of null values per row\",\"Count of Rows\"]\n",
    "count_df = count_df.sort_values([\"Number of null values per row\"], ascending=True)\n",
    "# count_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 17756 rows have complete data. I will run my the theano NN on it to fill the other ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110864"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (~df_train[\"v3\"].isnull()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from sklearn.base import BaseEstimator\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import cPickle as pickle\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "mode = theano.Mode(linker='cvm')\n",
    "#mode = 'DEBUG_MODE'\n",
    "\n",
    "\n",
    "class RNN(object):\n",
    "    \"\"\"    Recurrent neural network class\n",
    "    Supported output types:\n",
    "    real : linear output units, use mean-squared error\n",
    "    binary : binary output units, use cross-entropy error\n",
    "    softmax : single softmax out, use cross-entropy error\n",
    "    \"\"\"\n",
    "    def __init__(self, input, n_in, n_hidden, n_out, activation=T.tanh,\n",
    "                 output_type='real', use_symbolic_softmax=False):\n",
    "\n",
    "        self.input = input\n",
    "        self.activation = activation\n",
    "        self.output_type = output_type\n",
    "\n",
    "        # when using HF, SoftmaxGrad.grad is not implemented\n",
    "        # use a symbolic softmax which is slightly slower than T.nnet.softmax\n",
    "        # See: http://groups.google.com/group/theano-dev/browse_thread/\n",
    "        # thread/3930bd5a6a67d27a\n",
    "        if use_symbolic_softmax:\n",
    "            def symbolic_softmax(x):\n",
    "                e = T.exp(x)\n",
    "                return e / T.sum(e, axis=1).dimshuffle(0, 'x')\n",
    "            self.softmax = symbolic_softmax\n",
    "        else:\n",
    "            self.softmax = T.nnet.softmax\n",
    "\n",
    "        # recurrent weights as a shared variable\n",
    "        W_init = np.asarray(np.random.uniform(size=(n_hidden, n_hidden),\n",
    "                                              low=-.01, high=.01),\n",
    "                                              dtype=theano.config.floatX)\n",
    "        self.W = theano.shared(value=W_init, name='W')\n",
    "        # input to hidden layer weights\n",
    "        W_in_init = np.asarray(np.random.uniform(size=(n_in, n_hidden),\n",
    "                                                 low=-.01, high=.01),\n",
    "                                                 dtype=theano.config.floatX)\n",
    "        self.W_in = theano.shared(value=W_in_init, name='W_in')\n",
    "\n",
    "        # hidden to output layer weights\n",
    "        W_out_init = np.asarray(np.random.uniform(size=(n_hidden, n_out),\n",
    "                                                  low=-.01, high=.01),\n",
    "                                                  dtype=theano.config.floatX)\n",
    "        self.W_out = theano.shared(value=W_out_init, name='W_out')\n",
    "\n",
    "        h0_init = np.zeros((n_hidden,), dtype=theano.config.floatX)\n",
    "        self.h0 = theano.shared(value=h0_init, name='h0')\n",
    "\n",
    "        bh_init = np.zeros((n_hidden,), dtype=theano.config.floatX)\n",
    "        self.bh = theano.shared(value=bh_init, name='bh')\n",
    "\n",
    "        by_init = np.zeros((n_out,), dtype=theano.config.floatX)\n",
    "        self.by = theano.shared(value=by_init, name='by')\n",
    "\n",
    "        self.params = [self.W, self.W_in, self.W_out, self.h0,\n",
    "                       self.bh, self.by]\n",
    "\n",
    "        # for every parameter, we maintain it's last update\n",
    "        # the idea here is to use \"momentum\"\n",
    "        # keep moving mostly in the same direction\n",
    "        self.updates = {}\n",
    "        for param in self.params:\n",
    "            init = np.zeros(param.get_value(borrow=True).shape,\n",
    "                            dtype=theano.config.floatX)\n",
    "            self.updates[param] = theano.shared(init)\n",
    "\n",
    "        # recurrent function (using tanh activation function) and linear output\n",
    "        # activation function\n",
    "        def step(x_t, h_tm1):\n",
    "            h_t = self.activation(T.dot(x_t, self.W_in) + \\\n",
    "                                  T.dot(h_tm1, self.W) + self.bh)\n",
    "            y_t = T.dot(h_t, self.W_out) + self.by\n",
    "            return h_t, y_t\n",
    "\n",
    "        # the hidden state `h` for the entire sequence, and the output for the\n",
    "        # entire sequence `y` (first dimension is always time)\n",
    "        [self.h, self.y_pred], _ = theano.scan(step,\n",
    "                                               sequences=self.input,\n",
    "                                               outputs_info=[self.h0, None])\n",
    "\n",
    "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
    "        # be small\n",
    "        self.L1 = 0\n",
    "        self.L1 += abs(self.W.sum())\n",
    "        self.L1 += abs(self.W_in.sum())\n",
    "        self.L1 += abs(self.W_out.sum())\n",
    "\n",
    "        # square of L2 norm ; one regularization option is to enforce\n",
    "        # square of L2 norm to be small\n",
    "        self.L2_sqr = 0\n",
    "        self.L2_sqr += (self.W ** 2).sum()\n",
    "        self.L2_sqr += (self.W_in ** 2).sum()\n",
    "        self.L2_sqr += (self.W_out ** 2).sum()\n",
    "\n",
    "        if self.output_type == 'real':\n",
    "            self.loss = lambda y: self.mse(y)\n",
    "        elif self.output_type == 'binary':\n",
    "            # push through sigmoid\n",
    "            self.p_y_given_x = T.nnet.sigmoid(self.y_pred)  # apply sigmoid\n",
    "            self.y_out = T.round(self.p_y_given_x)  # round to {0,1}\n",
    "            self.loss = lambda y: self.nll_binary(y)\n",
    "        elif self.output_type == 'softmax':\n",
    "            # push through softmax, computing vector of class-membership\n",
    "            # probabilities in symbolic form\n",
    "            self.p_y_given_x = self.softmax(self.y_pred)\n",
    "\n",
    "            # compute prediction as class whose probability is maximal\n",
    "            self.y_out = T.argmax(self.p_y_given_x, axis=-1)\n",
    "            self.loss = lambda y: self.nll_multiclass(y)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def mse(self, y):\n",
    "        # error between output and target\n",
    "        return T.mean((self.y_pred - y) ** 2)\n",
    "\n",
    "    def nll_binary(self, y):\n",
    "        # negative log likelihood based on binary cross entropy error\n",
    "        return T.mean(T.nnet.binary_crossentropy(self.p_y_given_x, y))\n",
    "\n",
    "    def nll_multiclass(self, y):\n",
    "        # negative log likelihood based on multiclass cross entropy error\n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e.,\n",
    "        # number of time steps (call it T) in the sequence\n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain\n",
    "        # [0,1,2,... n-1] T.log(self.p_y_given_x) is a matrix of\n",
    "        # Log-Probabilities (call it LP) with one row per example and\n",
    "        # one column per class LP[T.arange(y.shape[0]),y] is a vector\n",
    "        # v containing [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ...,\n",
    "        # LP[n-1,y[n-1]]] and T.mean(LP[T.arange(y.shape[0]),y]) is\n",
    "        # the mean (across minibatch examples) of the elements in v,\n",
    "        # i.e., the mean log-likelihood across the minibatch.\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def errors(self, y):\n",
    "        \"\"\"Return a float representing the number of errors in the sequence\n",
    "        over the total number of examples in the sequence ; zero one\n",
    "        loss over the size of the sequence\n",
    "        :type y: theano.tensor.TensorType\n",
    "        :param y: corresponds to a vector that gives for each example the\n",
    "                  correct label\n",
    "        \"\"\"\n",
    "        # check if y has same dimension of y_pred\n",
    "        if y.ndim != self.y_out.ndim:\n",
    "            raise TypeError('y should have the same shape as self.y_out',\n",
    "                ('y', y.type, 'y_out', self.y_out.type))\n",
    "\n",
    "        if self.output_type in ('binary', 'softmax'):\n",
    "            # check if y is of the correct datatype\n",
    "            if y.dtype.startswith('int'):\n",
    "                # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "                # represents a mistake in prediction\n",
    "                return T.mean(T.neq(self.y_out, y))\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "\n",
    "class MetaRNN(BaseEstimator):\n",
    "    def __init__(self, n_in=5, n_hidden=50, n_out=5, learning_rate=0.01,\n",
    "                 n_epochs=100, L1_reg=0.00, L2_reg=0.00, learning_rate_decay=1,\n",
    "                 activation='tanh', output_type='real',\n",
    "                 final_momentum=0.9, initial_momentum=0.5,\n",
    "                 momentum_switchover=5,\n",
    "                 use_symbolic_softmax=False):\n",
    "        self.n_in = int(n_in)\n",
    "        self.n_hidden = int(n_hidden)\n",
    "        self.n_out = int(n_out)\n",
    "        self.learning_rate = float(learning_rate)\n",
    "        self.learning_rate_decay = float(learning_rate_decay)\n",
    "        self.n_epochs = int(n_epochs)\n",
    "        self.L1_reg = float(L1_reg)\n",
    "        self.L2_reg = float(L2_reg)\n",
    "        self.activation = activation\n",
    "        self.output_type = output_type\n",
    "        self.initial_momentum = float(initial_momentum)\n",
    "        self.final_momentum = float(final_momentum)\n",
    "        self.momentum_switchover = int(momentum_switchover)\n",
    "        self.use_symbolic_softmax = use_symbolic_softmax\n",
    "\n",
    "        self.ready()\n",
    "\n",
    "    def ready(self):\n",
    "        # input (where first dimension is time)\n",
    "        self.x = T.matrix()\n",
    "        # target (where first dimension is time)\n",
    "        if self.output_type == 'real':\n",
    "            self.y = T.matrix(name='y', dtype=theano.config.floatX)\n",
    "        elif self.output_type == 'binary':\n",
    "            self.y = T.matrix(name='y', dtype='int32')\n",
    "        elif self.output_type == 'softmax':  # only vector labels supported\n",
    "            self.y = T.vector(name='y', dtype='int32')\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # initial hidden state of the RNN\n",
    "        self.h0 = T.vector()\n",
    "        # learning rate\n",
    "        self.lr = T.scalar()\n",
    "\n",
    "        if self.activation == 'tanh':\n",
    "            activation = T.tanh\n",
    "        elif self.activation == 'sigmoid':\n",
    "            activation = T.nnet.sigmoid\n",
    "        elif self.activation == 'relu':\n",
    "            activation = lambda x: x * (x > 0)\n",
    "        elif self.activation == 'cappedrelu':\n",
    "            activation = lambda x: T.minimum(x * (x > 0), 6)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.rnn = RNN(input=self.x, n_in=self.n_in,\n",
    "                       n_hidden=self.n_hidden, n_out=self.n_out,\n",
    "                       activation=activation, output_type=self.output_type,\n",
    "                       use_symbolic_softmax=self.use_symbolic_softmax)\n",
    "\n",
    "        if self.output_type == 'real':\n",
    "            self.predict = theano.function(inputs=[self.x, ],\n",
    "                                           outputs=self.rnn.y_pred,\n",
    "                                           mode=mode)\n",
    "        elif self.output_type == 'binary':\n",
    "            self.predict_proba = theano.function(inputs=[self.x, ],\n",
    "                                outputs=self.rnn.p_y_given_x, mode=mode)\n",
    "            self.predict = theano.function(inputs=[self.x, ],\n",
    "                                outputs=T.round(self.rnn.p_y_given_x),\n",
    "                                mode=mode)\n",
    "        elif self.output_type == 'softmax':\n",
    "            self.predict_proba = theano.function(inputs=[self.x, ],\n",
    "                        outputs=self.rnn.p_y_given_x, mode=mode)\n",
    "            self.predict = theano.function(inputs=[self.x, ],\n",
    "                                outputs=self.rnn.y_out, mode=mode)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def shared_dataset(self, data_xy):\n",
    "        \"\"\" Load the dataset into shared variables \"\"\"\n",
    "\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(np.asarray(data_x,\n",
    "                                            dtype=theano.config.floatX))\n",
    "\n",
    "        shared_y = theano.shared(np.asarray(data_y,\n",
    "                                            dtype=theano.config.floatX))\n",
    "\n",
    "        if self.output_type in ('binary', 'softmax'):\n",
    "            return shared_x, T.cast(shared_y, 'int32')\n",
    "        else:\n",
    "            return shared_x, shared_y\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\" Return state sequence.\"\"\"\n",
    "        params = self._get_params()  # parameters set in constructor\n",
    "        weights = [p.get_value() for p in self.rnn.params]\n",
    "        state = (params, weights)\n",
    "        return state\n",
    "\n",
    "    def _set_weights(self, weights):\n",
    "        \"\"\" Set fittable parameters from weights sequence.\n",
    "        Parameters must be in the order defined by self.params:\n",
    "            W, W_in, W_out, h0, bh, by\n",
    "        \"\"\"\n",
    "        i = iter(weights)\n",
    "\n",
    "        for param in self.rnn.params:\n",
    "            param.set_value(i.next())\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\" Set parameters from state sequence.\n",
    "        Parameters must be in the order defined by self.params:\n",
    "            W, W_in, W_out, h0, bh, by\n",
    "        \"\"\"\n",
    "        params, weights = state\n",
    "        self.set_params(**params)\n",
    "        self.ready()\n",
    "        self._set_weights(weights)\n",
    "\n",
    "    def save(self, fpath='.', fname=None):\n",
    "        \"\"\" Save a pickled representation of Model state. \"\"\"\n",
    "        fpathstart, fpathext = os.path.splitext(fpath)\n",
    "        if fpathext == '.pkl':\n",
    "            # User supplied an absolute path to a pickle file\n",
    "            fpath, fname = os.path.split(fpath)\n",
    "\n",
    "        elif fname is None:\n",
    "            # Generate filename based on date\n",
    "            date_obj = datetime.datetime.now()\n",
    "            date_str = date_obj.strftime('%Y-%m-%d-%H:%M:%S')\n",
    "            class_name = self.__class__.__name__\n",
    "            fname = '%s.%s.pkl' % (class_name, date_str)\n",
    "\n",
    "        fabspath = os.path.join(fpath, fname)\n",
    "\n",
    "        logger.info(\"Saving to %s ...\" % fabspath)\n",
    "        file = open(fabspath, 'wb')\n",
    "        state = self.__getstate__()\n",
    "        pickle.dump(state, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        file.close()\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\" Load model parameters from path. \"\"\"\n",
    "        logger.info(\"Loading from %s ...\" % path)\n",
    "        file = open(path, 'rb')\n",
    "        state = pickle.load(file)\n",
    "        self.__setstate__(state)\n",
    "        file.close()\n",
    "\n",
    "    def fit(self, X_train, Y_train, X_test=None, Y_test=None,\n",
    "            validation_frequency=100):\n",
    "        \"\"\" Fit model\n",
    "        Pass in X_test, Y_test to compute test error and report during\n",
    "        training.\n",
    "        X_train : ndarray (n_seq x n_steps x n_in)\n",
    "        Y_train : ndarray (n_seq x n_steps x n_out)\n",
    "        validation_frequency : int\n",
    "            in terms of number of sequences (or number of weight updates)\n",
    "        \"\"\"\n",
    "        if X_test is not None:\n",
    "            assert(Y_test is not None)\n",
    "            self.interactive = True\n",
    "            test_set_x, test_set_y = self.shared_dataset((X_test, Y_test))\n",
    "        else:\n",
    "            self.interactive = False\n",
    "\n",
    "        train_set_x, train_set_y = self.shared_dataset((X_train, Y_train))\n",
    "\n",
    "        n_train = train_set_x.get_value(borrow=True).shape[0]\n",
    "        if self.interactive:\n",
    "            n_test = test_set_x.get_value(borrow=True).shape[0]\n",
    "\n",
    "        ######################\n",
    "        # BUILD ACTUAL MODEL #\n",
    "        ######################\n",
    "        logger.info('... building the model')\n",
    "\n",
    "        index = T.lscalar('index')    # index to a case\n",
    "        # learning rate (may change)\n",
    "        l_r = T.scalar('l_r', dtype=theano.config.floatX)\n",
    "        mom = T.scalar('mom', dtype=theano.config.floatX)  # momentum\n",
    "\n",
    "        cost = self.rnn.loss(self.y) \\\n",
    "            + self.L1_reg * self.rnn.L1 \\\n",
    "            + self.L2_reg * self.rnn.L2_sqr\n",
    "\n",
    "        compute_train_error = theano.function(inputs=[index, ],\n",
    "                                              outputs=self.rnn.loss(self.y),\n",
    "                                              givens={\n",
    "                                                  self.x: train_set_x[index],\n",
    "                                                  self.y: train_set_y[index]},\n",
    "            mode=mode)\n",
    "\n",
    "        if self.interactive:\n",
    "            compute_test_error = theano.function(inputs=[index, ],\n",
    "                        outputs=self.rnn.loss(self.y),\n",
    "                        givens={\n",
    "                            self.x: test_set_x[index],\n",
    "                            self.y: test_set_y[index]},\n",
    "                        mode=mode)\n",
    "\n",
    "        # compute the gradient of cost with respect to theta = (W, W_in, W_out)\n",
    "        # gradients on the weights using BPTT\n",
    "        gparams = []\n",
    "        for param in self.rnn.params:\n",
    "            gparam = T.grad(cost, param)\n",
    "            gparams.append(gparam)\n",
    "\n",
    "        updates = {}\n",
    "        for param, gparam in zip(self.rnn.params, gparams):\n",
    "            weight_update = self.rnn.updates[param]\n",
    "            upd = mom * weight_update - l_r * gparam\n",
    "            updates[weight_update] = upd\n",
    "            updates[param] = param + upd\n",
    "\n",
    "        # compiling a Theano function `train_model` that returns the\n",
    "        # cost, but in the same time updates the parameter of the\n",
    "        # model based on the rules defined in `updates`\n",
    "        train_model = theano.function(inputs=[index, l_r, mom],\n",
    "                                      outputs=cost,\n",
    "                                      updates=updates,\n",
    "                                      givens={\n",
    "                                          self.x: train_set_x[index],\n",
    "                                          self.y: train_set_y[index]},\n",
    "                                          mode=mode)\n",
    "\n",
    "        ###############\n",
    "        # TRAIN MODEL #\n",
    "        ###############\n",
    "        logger.info('... training')\n",
    "        epoch = 0\n",
    "\n",
    "        while (epoch < self.n_epochs):\n",
    "            epoch = epoch + 1\n",
    "            for idx in xrange(n_train):\n",
    "                effective_momentum = self.final_momentum \\\n",
    "                               if epoch > self.momentum_switchover \\\n",
    "                               else self.initial_momentum\n",
    "                example_cost = train_model(idx, self.learning_rate,\n",
    "                                           effective_momentum)\n",
    "\n",
    "                # iteration number (how many weight updates have we made?)\n",
    "                # epoch is 1-based, index is 0 based\n",
    "                iter = (epoch - 1) * n_train + idx + 1\n",
    "\n",
    "                if iter % validation_frequency == 0:\n",
    "                    # compute loss on training set\n",
    "                    train_losses = [compute_train_error(i)\n",
    "                                    for i in xrange(n_train)]\n",
    "                    this_train_loss = np.mean(train_losses)\n",
    "\n",
    "                    if self.interactive:\n",
    "                        test_losses = [compute_test_error(i)\n",
    "                                        for i in xrange(n_test)]\n",
    "                        this_test_loss = np.mean(test_losses)\n",
    "\n",
    "                        logger.info('epoch %i, seq %i/%i, tr loss %f '\n",
    "                                    'te loss %f lr: %f' % \\\n",
    "                        (epoch, idx + 1, n_train,\n",
    "                         this_train_loss, this_test_loss, self.learning_rate))\n",
    "                    else:\n",
    "                        logger.info('epoch %i, seq %i/%i, train loss %f '\n",
    "                                    'lr: %f' % \\\n",
    "                                    (epoch, idx + 1, n_train, this_train_loss,\n",
    "                                     self.learning_rate))\n",
    "\n",
    "            self.learning_rate *= self.learning_rate_decay\n",
    "\n",
    "\n",
    "def test_real():\n",
    "    \"\"\" Test RNN with real-valued outputs. \"\"\"\n",
    "    n_hidden = 10\n",
    "    n_in = 5\n",
    "    n_out = 3\n",
    "    n_steps = 10\n",
    "    n_seq = 100\n",
    "\n",
    "    np.random.seed(0)\n",
    "    # simple lag test\n",
    "    seq = np.random.randn(n_seq, n_steps, n_in)\n",
    "    targets = np.zeros((n_seq, n_steps, n_out))\n",
    "\n",
    "    targets[:, 1:, 0] = seq[:, :-1, 3]  # delayed 1\n",
    "    targets[:, 1:, 1] = seq[:, :-1, 2]  # delayed 1\n",
    "    targets[:, 2:, 2] = seq[:, :-2, 0]  # delayed 2\n",
    "\n",
    "    targets += 0.01 * np.random.standard_normal(targets.shape)\n",
    "\n",
    "    model = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n",
    "                    learning_rate=0.001, learning_rate_decay=0.999,\n",
    "                    n_epochs=400, activation='tanh')\n",
    "\n",
    "    model.fit(seq, targets, validation_frequency=1000)\n",
    "\n",
    "    plt.close('all')\n",
    "    fig = plt.figure()\n",
    "    ax1 = plt.subplot(211)\n",
    "    plt.plot(seq[0])\n",
    "    ax1.set_title('input')\n",
    "\n",
    "    ax2 = plt.subplot(212)\n",
    "    true_targets = plt.plot(targets[0])\n",
    "\n",
    "    guess = model.predict(seq[0])\n",
    "    guessed_targets = plt.plot(guess, linestyle='--')\n",
    "    for i, x in enumerate(guessed_targets):\n",
    "        x.set_color(true_targets[i].get_color())\n",
    "    ax2.set_title('solid: true output, dashed: model output')\n",
    "\n",
    "\n",
    "def test_binary(multiple_out=False, n_epochs=250):\n",
    "    \"\"\" Test RNN with binary outputs. \"\"\"\n",
    "    n_hidden = 10\n",
    "    n_in = 5\n",
    "    if multiple_out:\n",
    "        n_out = 2\n",
    "    else:\n",
    "        n_out = 1\n",
    "    n_steps = 10\n",
    "    n_seq = 100\n",
    "\n",
    "    np.random.seed(0)\n",
    "    # simple lag test\n",
    "    seq = np.random.randn(n_seq, n_steps, n_in)\n",
    "    targets = np.zeros((n_seq, n_steps, n_out))\n",
    "\n",
    "    # whether lag 1 (dim 3) is greater than lag 2 (dim 0)\n",
    "    targets[:, 2:, 0] = np.cast[np.int](seq[:, 1:-1, 3] > seq[:, :-2, 0])\n",
    "\n",
    "    if multiple_out:\n",
    "        # whether product of lag 1 (dim 4) and lag 1 (dim 2)\n",
    "        # is less than lag 2 (dim 0)\n",
    "        targets[:, 2:, 1] = np.cast[np.int](\n",
    "            (seq[:, 1:-1, 4] * seq[:, 1:-1, 2]) > seq[:, :-2, 0])\n",
    "\n",
    "    model = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n",
    "                    learning_rate=0.001, learning_rate_decay=0.999,\n",
    "                    n_epochs=n_epochs, activation='tanh', output_type='binary')\n",
    "\n",
    "    model.fit(seq, targets, validation_frequency=1000)\n",
    "\n",
    "    seqs = xrange(10)\n",
    "\n",
    "    plt.close('all')\n",
    "    for seq_num in seqs:\n",
    "        fig = plt.figure()\n",
    "        ax1 = plt.subplot(211)\n",
    "        plt.plot(seq[seq_num])\n",
    "        ax1.set_title('input')\n",
    "        ax2 = plt.subplot(212)\n",
    "        true_targets = plt.step(xrange(n_steps), targets[seq_num], marker='o')\n",
    "\n",
    "        guess = model.predict_proba(seq[seq_num])\n",
    "        guessed_targets = plt.step(xrange(n_steps), guess)\n",
    "        plt.setp(guessed_targets, linestyle='--', marker='d')\n",
    "        for i, x in enumerate(guessed_targets):\n",
    "            x.set_color(true_targets[i].get_color())\n",
    "        ax2.set_ylim((-0.1, 1.1))\n",
    "        ax2.set_title('solid: true output, dashed: model output (prob)')\n",
    "\n",
    "\n",
    "def test_softmax(n_epochs=250):\n",
    "    \"\"\" Test RNN with softmax outputs. \"\"\"\n",
    "    n_hidden = 10\n",
    "    n_in = 5\n",
    "    n_steps = 10\n",
    "    n_seq = 100\n",
    "    n_classes = 3\n",
    "    n_out = n_classes  # restricted to single softmax per time step\n",
    "\n",
    "    np.random.seed(0)\n",
    "    # simple lag test\n",
    "    seq = np.random.randn(n_seq, n_steps, n_in)\n",
    "    targets = np.zeros((n_seq, n_steps), dtype=np.int)\n",
    "\n",
    "    thresh = 0.5\n",
    "    # if lag 1 (dim 3) is greater than lag 2 (dim 0) + thresh\n",
    "    # class 1\n",
    "    # if lag 1 (dim 3) is less than lag 2 (dim 0) - thresh\n",
    "    # class 2\n",
    "    # if lag 2(dim0) - thresh <= lag 1 (dim 3) <= lag2(dim0) + thresh\n",
    "    # class 0\n",
    "    targets[:, 2:][seq[:, 1:-1, 3] > seq[:, :-2, 0] + thresh] = 1\n",
    "    targets[:, 2:][seq[:, 1:-1, 3] < seq[:, :-2, 0] - thresh] = 2\n",
    "    #targets[:, 2:, 0] = np.cast[np.int](seq[:, 1:-1, 3] > seq[:, :-2, 0])\n",
    "\n",
    "    model = MetaRNN(n_in=n_in, n_hidden=n_hidden, n_out=n_out,\n",
    "                    learning_rate=0.001, learning_rate_decay=0.999,\n",
    "                    n_epochs=n_epochs, activation='tanh',\n",
    "                    output_type='softmax', use_symbolic_softmax=False)\n",
    "\n",
    "    model.fit(seq, targets, validation_frequency=1000)\n",
    "\n",
    "    seqs = xrange(10)\n",
    "\n",
    "    plt.close('all')\n",
    "    for seq_num in seqs:\n",
    "        fig = plt.figure()\n",
    "        ax1 = plt.subplot(211)\n",
    "        plt.plot(seq[seq_num])\n",
    "        ax1.set_title('input')\n",
    "        ax2 = plt.subplot(212)\n",
    "\n",
    "        # blue line will represent true classes\n",
    "        true_targets = plt.step(xrange(n_steps), targets[seq_num], marker='o')\n",
    "\n",
    "        # show probabilities (in b/w) output by model\n",
    "        guess = model.predict_proba(seq[seq_num])\n",
    "        guessed_probs = plt.imshow(guess.T, interpolation='nearest',\n",
    "                                   cmap='gray')\n",
    "        ax2.set_title('blue: true class, grayscale: probs assigned by model')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    t0 = time.time()\n",
    "    test_real()\n",
    "    # problem takes more epochs to solve\n",
    "    #test_binary(multiple_out=True, n_epochs=2400)\n",
    "    #test_softmax(n_epochs=250)\n",
    "    print \"Elapsed time: %f\" % (time.time() - t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
